---
layout:     post
title:      "这就是搜索引擎第一章"
subtitle:   "这就是搜索引擎读书笔记"
date:       2016-05-15 21:00:00
author:     "秀川"
header-img: "img/post-bg-re-vs-ng2.jpg"
header-mask: 0.3
catalog:    true
tags:
    - 读书笔记
    - 《这就是搜索引擎》
    - 搜索
---

> t

## 搜索引擎发展史

#### 史前时代
采用分类目录的方式，一般被收录的网站质量都较高，但是这种方式可扩展性不强，绝大部分网站不能被收录；纯手工录入，没有用高深的技术。

#### 第一代：文本检索的一代
第一代搜索引擎使用信息检索模型，比如布尔模型，向量空间模型或者概率模型来计算用户查询关键字和网页内容的相关程度，网页之间丰富的链接信息，在这一代搜索引擎被没有被使用，相比分类目录，这一代搜索引擎可以收录大部分的网页，并能够按照网页内容和用户查询匹配程度进行排序，但是总体搜索质量并不是很好。

#### 第二代：链接分析的一代
网页链接代表了一种推荐关系，所以通过链接分析可以在海量的内容中找出重要的网页，这种重要性本质上是对流行程度的一种衡量，因为被推荐次数多的网页其实代表了其具有流行性，搜索引擎可以通过流行性和内容相似性来改善搜索质量

Google率先提出[PageRank链接分析技术]()，并大获成功，后来学术界陆续提出了很多改进的链接分析算法，几乎所有的商用搜索引擎都采取了链接分析技术。

采用链接分析能够改善搜索结果，但是这种搜索引擎并没有考虑用户个性化要求，所以只要查询请求相同，所有用户都会获得相同的搜索结果，另外很多网站可以针对链接分析算法提出了不少链接作弊方案来提高搜索排名，这样导致搜索质量变差。

#### 第三代：用户中心的一代
目前搜索引擎大都可以归为第三代，即以理解用户需求为核心。不同用户输入同一个关键词，但其目的可以能不一样；同一个用户两次查询同一个关键词也有可能因为时间地点的不同而需求有所变化。

目前搜索引擎大都致力于以下问题：如何能够理解用户发出的某个很短小的查询词背后包含的正真需求，所以这一代搜索引擎称之为以用户为中心的一代。

## 搜索引擎的三个目标

搜索引擎的应用非常简单：用户输入查询词，搜索引擎返回搜索结果，但是要以亿计数的互联网用户提供准确快速的搜索结果，里面包含了很多技术手段。

> 更全，更快，更准

所谓**更全**，是从其索引的网页数量而言，目前任意一个商业搜索引擎索引的网页的覆盖范围都只占互联网页面的一部分，可以通过提高[网络爬虫]()相关技术来达到此目的。

**更快**这个目标则贯穿于搜索引擎的大多数技术方向，比如索引相关技术，缓存等技术的提出都是直接为了达到此目的。而其他很多技术也间接为此服务，即使是分布式海量云存储平台，也是为了能够处理海量的网页数据，以达到对**更全**和**更快**这两个目标的响应和支持。

对一个搜索引擎来说，**更全**和**更快**可以使其不落后与同类产品，但是如果能够做到**更准**，则能够构建核心竞争能力。

#### 搜索引擎部分技术与目标

使用技术 | 更快 | 更全 | 更准
:------------: | :-------------: | :------------: | :------------:
索引 |   | √ | 
索引压缩 |   | √ | 
排序 |   |  | √
链接分析 |   |  | √
反作弊 |   |  | √
用户研究 |   |  | √
云存储 | √  | √ | 
爬虫 | √  | √ | 
网页去重 |   | √ | √
缓存 |   | √ | 

## 搜索引擎的三个核心问题

搜索引擎如何能够更准确是其最重要的目标，那么如何使得搜索结果更准确？涉及到3个核心问题

#### 一、用户的正真需求是什么
搜索引擎用户输入的查询请求非常简短，查询的平均长度是2.7个单词。如何从如此短的查询请求里获知隐藏其后的真实用户需求，这是搜索引擎首先需要解决的非常重要的问题。

同一个查询，不同用户的搜索目的是不同的，如何识别这种差异？更进一步，同一个用户发出的同一个查询词，也可能因为用户所处的场景不同，且目的存在差异，又如何识别？所有这些都是搜索引擎需要解决的核心问题。

#### 二、哪些信息是和用户需求正真相关的
第一个核心问题是从用户需求角度出发的，另外两个是从数据角度考虑的。搜索引擎本质是一个匹配过程，即从海量数据里找到能够匹配用户需求的内容，所以在明确用户真实意图这个前提条件做到后，如何找到能够满足用户需求的信息则成为关键因素。

#### 三、哪些信息是用户可以信赖的
搜索本质是找到能够满足用户需求的信息，尽管相关性是衡量信息是否满足用户需求的一个重要方面，但并非全部，信息是否值得信赖是另一个重要的衡量标准。

> 比如一个用户到某一个餐馆就餐，在作出消费决定前，在网上搜索曾在此就餐的用户的过往评论，以此辅助决策。而搜索到的相关内容，完全有可能是餐馆故意发布的一些好评信息，以此误导消费者，但是如果信息发布者是该用户的朋友，那么信息的可信性就会大大增加。

从某种角度看，链接分析之所以能够改善搜索结果，可以认为是对信息的可信程度作出的评判，即将网页的重要性做为是否可信赖的一个判断标准，返回重要网页即是返回可信赖网页。

## 搜索引擎的技术架构

做为互联网应用中最具有技术含量的应用之一，优秀的搜索引擎需要复杂的架构和算法，以此来支撑对海量数据的获取，存储，以及对用户查询的快速而准确的响应。

从架构的层面，搜索引擎需要能够对百亿计的海量网页进行获取、存储、处理的能力，同时保证搜索的结果的质量。如何获取、存储并计算如此海量的数据？如何快速响应用户的查询？如何使得搜索结果能够满足用户的信息需求？这些都是搜索引擎面对的技术挑战。

以下是一个通用的搜索引擎架构示意图：

搜索引擎的信息来源于互联网网页，通过网络爬虫将整个互联网的信息获取到本地，因为互联网页面很大比例的内容是完全相同或者相似的，**网页去重**模块会对此作出检测，并且除去重复页面。

在此之后，搜索引擎会对网页进行解析，抽取出网页主体内容，以及页面中包含的指向其他页面的链接。为了加快响应用户查询速度，网页内存通过**倒排索引**这种高效查询数据结构来保存，而网页之间的链接关系也会保存。之所以要保存链接关系，是因为这种关系会在网页相关性排序阶段是可利用的，通过**链接分析**可以判断页面的相对重要性，对于用户提供准确的搜索结果帮助很大。




