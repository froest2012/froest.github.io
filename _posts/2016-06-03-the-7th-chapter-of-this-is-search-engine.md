---
layout:     post
title:      "这就是搜索引擎第六章-云存储和云计算"
subtitle:   "这就是搜索引擎读书笔记"
date:       2016-05-29
author:     "秀川"
header-img: "img/post-bg-re-vs-ng2.jpg"
header-mask: 0.3
catalog:    true
tags:
    - 读书笔记
    - 《这就是搜索引擎》
    - 搜索
    - 云存储
    - 云计算
---

# 云存储与云计算
当数据超过百亿以后，如果使这些数据的存储和管理变得简单化，就成为了重要的问题，云存储以及云计算就是为了解决这类问题而提出的解决方案。

## 云存储与云计算概述
### 基本假设
云存储与云计算是由大型互联网公司提倡并推行，是由于互联网数据的爆炸性增长导致的；云存储和云计算平台的设计和构建都是基于以下一些基本假设和要求：

* 有大量廉价PC组成：一方面廉价PC可以减少企业成本，另一方面，商用数据库的扩展性无法满足互联网企业的要求。
* 机器节点出现故障时常态：PC的故障率很高，所以云存储和云计算平台在设计之初就得把这种经常性的故障做为一个设计考虑因素，在这个约束前提下提供可靠的服务，必须要考虑到数据的可用性和安全性。
* 水平增量扩展：互联网服务随时都要响应用户的请求，随着数据量的不断增大，需要靠增加机器来承载更多的数据，即使这样，也不可能中断服务来进行，所以必须在用户无察觉的情况下实现增量扩展，而对于云存储和云计算平台，应该做到简单增加机器就可以自动实现增量扩展，这往往是通过数据的水平分割实现的。
* 弱数据一致性：云存储与云计算平台有其优势：高可用性，易扩展性，容错性能好，但这种优势是有代价的，这个代价就是数据的弱一致性。
* 读多写少型服务：互联网服务有个特点，即读多写少，所以在设计云存储和云计算平台时，应该考虑到这个特点，作出有针对性的优化，保证系统效率

### 理论基础
CAP、BASE、ACID及最终一致性等这些基础原理对于深入理解云存储与云计算技术有重要指导作用。

CAP是对Consistency/Available/Partition Tolerance的一种简称，其分别代表一致性，可用性和分区容忍性。研究者已经证明对于一个数据系统来说，CAP3个要素不可兼得，同一个系统之多只能实现其中的两个，而必须放宽第3个要素来保证其他两个要素被满足。

对于分布式系统来说，分区容忍性是天然具备的要求，所以设计技术方案时，必须在高可用性和弱数据一致性中选择一个。传统的数据库牺牲了扩展性来实现强数据一致性和高可用性。`云存储系统往往关注高可用性和高可扩展性`，但是以弱数据一致性做为代价。

ACID是关系型数据库采纳的原则，也是一种简称。

* 原子性（Atomicity）：一个事务要么全部执行，要么完全不执行
* 一致性（Consistency）：事务开始和结束时，应该满足一致性约束条件。比如系统要求A+B=100，那么事务如果改变了A的数值，则B的数值也要相应修改来满足这种一致性要求。
* 事务独立（Isolation）：如果有多个事务同时执行，彼此之间不需要知晓对方的存在，而且执行时相互不影响，不允许出现两个事务交错，间隔执行部分任务的情形。
* 持久性（Durability）：事务的持久性是指事务运行成功以后，对系统状态的更新是永久的，不会无缘由的回滚撤销。

BASE原则是和ACID原则相反的基本原理。

* 基本可用（Basically Available）：在绝大多数时间内系统处于可用状态，允许偶尔的失败。
* 软状态或则柔性状态（Soft State）：数据状态不要求在任意时刻都完全保持同步。
* 最终一致性（Eventual Consistency）：与强数据一致性相比，最终一致性是一种弱数据一致性，尽管软状态不要求任意时刻保持一致同步，但是最终一致性要求在给定时间内数据到达一致状态。

BASE原则和ACID原则不同，是通过牺牲强数据一致性来获得高可用性的。尽管大多数云存储系统采纳了BASE原则，但是云存储系统的发展过程正在向逐步提供局部ACID特性发展，即全局而言符合BASE原则，但是局部支持ACID原则，这样就可以吸取两者各自的好处，在两者之间建立平衡，从Google的MegeStore可以看出这种发展趋势。

大多数云存储系统采用最终一致性，在分布式存储架构中，每份数据都要保留多个备份，但是由于客户端程序是并发对数据库`读/写`，可能同时有多个客户端将数据更新到不同的备份中，这可能导致数据的状态不一致，如何维护数据保持一致是一个核心问题。所谓强数据一致性，就是要求后续的读取操作看到的都是最新更新的数据；而弱数据一致性则放宽条件，允许读取到较旧版本的数据，最终一致性是给出一个时间窗口，在一段时间后，系统能够保证所有备份数据的更新是一致的。

### 数据模型
比较常见的数据模型有两种：Key/Value模式和模式自由（Schema Free）列表模式。

Key/Value模式每个记录有两个域构成，一个是主键，做为记录的唯一标识，另一个存储记录的数据值；

模式自由列表每个记录也有一个唯一的主键标识，不同点在于数据值，类似于关系数据库，数据由若干个列属性构成，但是与数据库不同的是：数据库一旦确定哪些属性，就固定不变，而云存储系统则不受此约束，可以随时增加或者删除某个列属性，同时每一行只有存储部分列属性，不必完全存储。

![云存储数据模型](云存储数据模型.png "云存储数据模型")
在云存储系统内部实现存储结构的时候，最终可以归结为两种实现方式；哈希加链表或者B+树的方式。哈希加链表查询速度较快，但是一次只能查询一条记录，无法支持批量属性查找记录（Scan方式），而B+树则支持这种查找方式，但是其管理方式相对复杂，索引这两种方式合适不同的应用场合。

### 基本问题
对于由大量廉价PC构成的分布式存储系统来说，不管具体方案采取何种技术路线，都面临一些问题，这些问题包括：

* 数据如何在机器之间分布

由于数据量巨大，单个机器很难承担存储全部数据的责任，所以必须将数据进行切割，将大数据切割成小份，并将其分配到其他机器上，数据的分布策略是首先要考虑的问题。

* 多备份数据如何保证一致性

在由大量廉价PC构成的分布式存储系统中，随时都可能有PC出现故障，放置在故障机器上的数据会因此不可以，处于可用性方面的考虑，现代云存储系统必须将数据做多备份，并将备份数据放置在不同机器上。而云计算环境下，很多客户端程序会并发对数据进行读/写，这样数据的多个备份之间如何保证其状态是一致的就成了云存储系统的核心问题。

* 如何响应客户端的读/写请求

几千台机器构成的云存储与云计算平台，对于客户端数据读/写请求，如何将请求在不同功能的机器之间转发，并做出针对性响应，同时读/写延迟尽可能短，数据尽可能保证正确，这些都是云存储与云计算平台需要提供的基本功能和保证。

* 加入（或者坏掉）一台机器如何处理

如果新加入存储机，系统应该将之纳入管理范围，并自动为其分配任务；同样的因机器故障不可用，如何识别这种状态及如何对其他负责存储的数据进行迁移与备份，这也是云存储平台的重要问题。

* 如何在机器之间进行负载均衡

大量PC共同承担数据存储与读写响应，很容易出现分工不均的情况，导致一些机器非常繁忙，而有些机器很空闲，这样繁忙机器很容易称为系统瓶颈，而闲置机器又没有充分利用。所以如何在机器之间进行负载均衡，使得不会出现瓶颈节点的同事又能充分利用机器资源。

### Google的云存储与云计算架构
![Google的云存储与云计算架构](Google的云存储与云计算架构.png "Google的云存储与云计算架构")

云存储技术：GFS文件系统，CHubby锁服务，BigTable及MegaStore等一系列不断进化的存储系统

云计算技术：MapReduce，Percolator及Prigel等互补的计算模式

## Google的文件系统
GFS是Google公司为了存储以百亿计的海量网页信息而专门开发的文件系统。

### GFS设计原则

1. 由于GFS采用大量商业PC来构建存储集群，所以机器发生硬盘故障或者死机是一种常态，因此数据冗余备份，自动检测机器是否还在有效提供服务，故障机器的自动恢复等都在GFS的设计目标里。
2. GFS文件系统所存储的文件绝大多数是大文件，所以系统对这种大文件的读/写优化。如果用这个存小文件，就牛刀小用了。
3. 系统存在大量追加操作，新增内容追加到文件的末尾，已经写入的数据一般不做更改。
4. 一般是顺序读取大量数据，很少随机读取少量数据。

### GFS整体架构
GFS由主控服务器（Master），总多的Chunk服务器和GFS客户端构成。尽管上千条机器构成GFS文件系统，但是在应用开发者严重，GFS就跟本地统一文件系统一样，分布式存储系统的细节对应用开发者来说是不可见的。

* 主控服务器：主要做管理工作
* Chunk服务器：负责实际数据的存储并响应GFS客户端的读写请求
* GFS客户端：负责向Chunk服务器发送读写请求

GFS文件系统由目录和存放在某个目录的文件构成的树形结构，称为GFS命名空间。GFS为应用开发者提供了文件的创建、删除、读取和写入等常见的操作API。

![GFS的文件存储示意图](GFS的文件存储示意图.png "GFS的文件存储示意图")
GFS存储的一般都是大文件，每个大文件会被切分成固定大小(一般64M)的Chunk数据块，每个Chunk数据块由多个block组成。

GFS以Chunk为存储单位，这是文件读取的基本单位，一次至少读取一个block。

![GFS整体架构](GFS整体架构.png "GFS整体架构.png")

基本流程：
* GFS客户端接收到用户读取文件的请求，从P位置读取大小为L的数据，通过位置P可以知道文件信息在哪个Chunk中，请求转换成<文件名， Chunk序号>的形式。
* GFS系统把这个请求发送给GFS主控服务器，主控服务器将Chunk序号转换成系统内唯一编号，把这两个信息返回给GFS客户端。
* GFS客户端拿到访问那台Chunk服务器之后，直接连接Chunk服务器，这样一次数据读取就完成了。

### GFS主控服务器
Google的云存储平台采用大量主从结构，即单一的主控服务器和众多的存储服务器。主控服务器主要从事系统元素局的存储管理及整个分布式系统的管理，比如负载均衡，数据在存储服务器之间迁移，检查新加入的机器及失效机器等工作。

采用主从结构的好处是整个系统存在一个全局的主控节点，所以管理起来相对简单。缺点是因为主控节点是唯一的，容易造成整个系统的瓶颈，还可能存在单点失效问题。

![GFS主控服务器所管理的系统数据](GFS主控服务器所管理的系统数据.png "GFS主控服务器所管理的系统数据")

1. GFS命名空间和Chunk命名空间:主要用来对目录文件及Chunk的增删改等信息进行记录。
2. 从文件到其所属Chunk之间的映射关系：因为一个文件会被切割成众多Chunk，所以系统需要维护这种信息
3. 每个Chunk在哪台Chunk服务器存储的信息：每个文件会被切分成多个Chunk，同时每个Chunk会被复制多个备份，并存储在不同服务器上，如果发生机器故障，可以在其他机器上找到对应的Chunk备份信息，云存储平台必须提供这种数据冗余来保证数据的安全。

GFS管理数据非常重要，必须得到保障，如果管理数据丢失，那么这个GFS系统也就不可用，所以GFS将前两类管理信息（命名空间及文件到Chunk映射表）记录在系统日志内。并且将这个系统日志分别存储在多台机器上，这样就避免了信息丢失问题。对于第3类管理数据（Chunk存储在哪台服务器的信息），主控服务器在启动时询问每个Chunk服务器，之后靠定期询问来保持最新信息（因为机器如果故障，就可能使用备份机器）。

主控服务器不仅存储管理系统原信息，还承担一些系统管理工作：比如创建新Chunk及其备份数据，不同Chunk服务器之间的负载均衡，如果某个Chunk不可用，则负载重新生成这个Chunk对应的备份数据，以及垃圾回收等工作。

在对数据进行备份和迁移工作室，GFS重点考虑：
* 一个是Chunk数据的可用性，如果发现Chunk数据不可用，要及时重新备份，以免某个Chunk的所有备份都不可用导致数据丢失；
* 另一个要尽可能减少网络传输压力。

### 系统交互行为
GFS执行写操作流程。

![GFS写操作](GFS写操作.png "GFS写操作")

* GFS客户端首先和主控服务器通信，获知哪些服务器存储了要写入的Chunk，包括主备份和两个次级备份的地址数据。
* GFS客户端将要写入的数据推送给3个Chunk，Chunk首先将待写入数据放在缓存中，然后通知GFS客户端是否接收成功
* 如果所有的备份都接收成功，GFS客户端通知主备份可以执行写入操作，主备份将自己缓存的数据写入Chunk中，通知次级备份按照指定顺序写入数据，次级备份写完后答复主备份写入成功，主备份会通知GFS客户端这次写操作成功完成。

## Chubby锁服务
Chubby是Google公司研发的针对分布式系统资源管理的粗粒度锁服务，一个Chubby实例大约可以负责1万台4核CPU机器之间对资源的协同管理。这种锁服务主要的功能是让总舵客户端程序进行相互之间的同步，并对系统环境或者资源达成一致认知。

Chubby类似于文件系统的目录和文件管理系统，并在此基础上提供针对文件的锁服务，Chubby的文件主要存储一些管理信息或者基础数据，Chubby要求的目的不是数据存储，而是对资源的同步管理，所以不推荐在文件中保存大量数据。


## BigTable
## MegaStore系统
## Map/Reduce云计算模型
## 咖啡因系统--Percolator
## Pregel图形计算模型
## Dynomo云存储系统
## PNUTS云存储系统
## HayStack存储系统