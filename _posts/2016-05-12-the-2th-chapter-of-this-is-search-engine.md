---
layout:     post
title:      "这就是搜索引擎第二章"
subtitle:   "这就是搜索引擎读书笔记"
date:       2016-05-14
author:     "秀川"
header-img: "img/post-bg-re-vs-ng2.jpg"
header-mask: 0.3
catalog:    true
tags:
    - 读书笔记
    - 《这就是搜索引擎》
    - 搜索
    - 网络爬虫
---

# 网络爬虫

网络爬虫的目的就是为了将海量的网页数据高效的下载到本地，在本地形成互联网的镜像备份。

### 通用爬虫框架

一个通用的爬虫框架模型，首先从互联网页面中精心挑选一部分网页做为种子url，将这些种子url放入待抓取url队列中，爬虫从待抓取url队列中依次读取，并将url通过dns解析，把链接地址转换为网站服务器对应的ip地址，然后将其和网页相对路径名称交给网页下载器，网页下载器负责页面内容的下载，对于下载到的本地网页，一方面将其存储到页面库中，等待建立索引等后续处理；另一方面将下载网页的url放入一抓取url队列中，这个队列记载了爬虫系统已经下载过的网页url，以避免网页的重复抓取。对于刚下载的网页，从中抽取出包含的所有链接信息，并在抓取url队列中检查，如果发现还没有抓取过，则将这个url放入待抓取url列表，在之后的抓取调度中会下载这个url对应的网页，如此这般，形成循环，知道待抓取url列表为空，这代表着爬虫系统已将能够抓取的顽固尽数抓完，此时完成了一轮完整的抓取过程。

如下图：

爬虫与互联网所有网页之间的关系可以划分为以下5个部分：

* 已下载网页集合
* 已过期网页集合	网页更新了以后，之前下载的页面就过期了
* 待下载网页集合
* 可知网页集合
* 不可知网页集合	无法被爬虫下载的网页被称为不可知网页，比如**暗网**中的页面

##### 爬虫类型
* 批量型爬虫

批量型爬虫有比较明确的抓取范围和目标，当完成设定的目标之后，即停止抓取过程。
* 增量型爬虫

增量型爬虫会保持不断的抓取，对于已经抓取到的网页，要定期更新，因为互联网的页面处于不断的变化中。通用的商用搜索引擎基本都属于此类。
* 垂直型爬虫

垂直型爬虫关注特定主体或者属于特定行业的网页。垂直型爬虫的一个最大的特点和难点就是，如何识别网页内容是否属于指定行业或者主体。垂直搜索网站往往需要此类爬虫。

### 优秀爬虫的特性
* 高性能

互联网的网页数量庞大如海，所以爬虫的性能至关重要。这里的性能主要是指爬虫下载网页的抓取速度。单位时间能够抓取的网页越多，性能越高

要提高爬虫的性能，在设计时程序访问磁盘的操作方法以及具体实现时数据结构的选择很关键。
* 可扩展性

爬虫需要抓取的网页数量巨大，即使单个爬虫性能很高，要将所有的网页下载到本地，仍然需要相当长的世界周期，为了能够尽可能缩短抓取周期，爬虫系统应该有很好的可扩展性，即很容易通过增加抓取服务器和爬虫数量来达到此目的。
* 健壮性

爬虫要访问各种类型的网站服务器，可能会遇到很多种非正常情况，比如网页html编码不规范，被抓去服务器突然死机，甚至是爬虫陷阱等，爬虫对应各种异常情况能够正确处理非常重要，否则可能会不定期停止工作，这是无法忍受的
* 友好性

爬虫的友好性包含两个方面的含义：一是保护网站的部分私密性，二是减少抓取网络的网络负载。

对于网络拥有者来说，可以有两种方法来让爬虫知道哪些内容是不允许抓取的：**爬虫紧抓协议**和**网页紧抓标记**。

爬虫禁抓协议(Robot Exclusion Protocol)：网站所有者生成一个指定的robot.txt文件，并放在网站服务器的根目录下，在这个文件中指明哪些目录下的网页是不允许爬虫抓取的。一般以目录为单位,如下图：
![爬虫禁抓协议写法](../../../../img/in-post/0512/robot.png "爬虫禁抓协议写法")

网页禁抓标记（Robot META Tag）：在网页的HTML代码中加入<meta name="robots" content="noindex">, 如下图：
![网页禁抓协议写法](../../../../img/in-post/0512/robotmeta.png "网页禁抓协议写法")

如果爬虫访问网站频率过高，会给网站服务器造成很大的访问压力，有时候甚至会影响网站的正常访问，造成类似DDOS攻击的效果。所以为了减少网站的负载，友好性的爬虫应该在抓取策略部署时考虑抓取网站的负载，尽可能不影响爬虫性能的情况下，减少对单一站点短期内的高频访问。

### 爬虫质量评价标准
上面介绍的是从爬虫开发者的角度考虑的，那么从搜索引擎用户的角度考虑，对爬虫的工作效果有不同的评价标准，最主要的三个标准是：抓取网页覆盖率，抓取网页时新性，抓取网页重要性。

抓取网页覆盖率越高，等价于搜索引擎的召回率越高，用户体验越好。

抓取时新性就是在网页库中过期的数据越少，则网页的时新性越好。

如果搜索引擎爬虫抓回的网页大都是比较重要的网页，则可以说在抓取网页的重要性方面做得很好，这方面做得好，等价于搜索引擎的搜索精度高。

> 尽可能选择比较重要的那部分页面来索引，对已经抓取到的网页尽可能快的更新其内容，是的索引网页和户联网对应页面内容同步更新，在此基础上，尽可能扩大抓取范围，抓取到更多以前无法发现的网页。

### 抓取策略
网页的重要性评价标准则是抓取策略的核心

需要下载的网页url放入待下载url队列，调度器每次从队列头取出某个url，发送给网页下载器下载页面内容，每个新下载的页面包含的url会追加到待下载url队列的末尾，如此形成循环，整个爬虫可以说是这个队列驱动运转的。

待抓取url队列中页面url顺序如何确定？不同的抓取策略，就是利用不同的方法确定待抓取url队列中url的优先顺序。

爬虫抓取策略的目标都是优先选择重要的网页进行抓取。以下是4中效果较好的爬虫策略：

##### 宽度优先遍历策略
什么是宽度优先策略？网页下载器下载了网页以后把网页中的url依次放入待下载列表队列的尾部，下载器按照待下载队列的顺序下载页面，这种方法就是宽度优先遍历策略，其实他什么都没有优化。但是据说效果还是可以的，如果某个网页包含很多入链，那么可能被宽度优先遍历策略早早的抓到，而入链的个数从侧面体现了网页的重要性，即宽度优先遍历策略隐含了一些网页优先级假设，如下图：
![宽度优先遍历策略](../../../../img/in-post/0512/宽度优先遍历策略.jpg "宽度优先遍历策略")

##### 非完全PageRank策略
PageRank算法是一个全局性算法，即当所有页面都下载完成后，其计算结果才是可靠的，而爬虫的目的就是去下载网页，在运行过程中只能看到部分页面，所以在抓取阶段的网页是无法得到可靠的PageRank得分的。

非完全PageRank策略基本思路：对于已经下载的网页，加上待抓取的url队列中的url一起，形成网页集合，在此集合内进行PageRank计算，计算完成后，将**待抓取url队列**里的网页按照PageRank得分由高到低排序，形成的序列就是爬虫接下来应该依次抓取的url列表，这也是称为**非完全PageRank**的原因。

如果每次新抓到一个网页，就将所有已经下载的网页重新计算新的非完全PageRank值，明显效率太低，在现实中是不可行的，一个折中的办法是，每当新下载的网页攒够K个，然后将所有下载页面重新计算一遍新的非完全PageRank，这样计算效率勉强可以。

但是又引来新的问题，在展开下一轮PageRank计算之前，重新下载的网页抽取出包含的链接，很有可能这些链接的重要性非常高，理应优先下载，这种情况怎么解决呢？

**非完全PageRank**赋予这些新抽取出来但是有没有PageRank值的网页一个临时PageRank，将这个网页的所有入链传到的PageRank值汇总，做为临时PageRank值，如果这个值比待抓取url队列中已经计算出来的PageRank值网页高，那么优先下载这个url
##### OCIP策略
OCIP策略是一种改进的PageRank算法，在这个算法开始之前，每个互联网页面给与相同的“现金（Cash）”， 每当下载了某个页面P后，P将自己拥有的“现金”评价分配给页面中包含的链接页面，把自己的“现金”清空，而对于待抓取url队列中的网页，则根据其手头拥有的现金金额多少排序，优先下载现金最充裕的网页，OCIP从大框架上与PageRank思路基本一致，区别在于：PageRank每次需要迭代计算，而OCIP策略不需要迭代过程，所以计算速度远远快于PageRank，适合实时计算使用。同时PageRank计算时，存在向无链接关系网页的远程跳转过程，而OCIP则没有这一计算因子。实验证明，OCIP是一种较好的重要性衡量策略，效果略优于宽度优先遍历策略。

##### 大站优先策略
大战优先策略思路是根据网络单位来衡量网页的重要性，对于抓取url队列张的网页，根据所属网站归类，如果哪个网站等待下载的页面多，则优先下载这些链接，其本质思想倾向于优先下载大型网站，因为大型完整往往包含更多页面

### 网页更新策略
网页更新策略为了增加下载网页的时效性，互联网的动态性是其显著的特征，随时都有新出现的页面，页面内容被更改或者本来存在的页面被删除，而爬虫爬取的只是页面的镜像，所以需要经常更新页面。

网页更新策略有如下几种：

* 历史参考策略

历史更新策略建立于一个假设之上：过去频繁更新的网页，将来也会频繁跟新，这种方法往往用**泊松过程**来对网页的变化进行建模

* 用户体验策略

用户体验策略就是利用搜索引擎的用户的特点来设计更新策略，这种更新以用户体验为核心，即使本地索引的网页是过时的，但如果不影响用户体验，那更新这些网页的优先级可以降低。所以何时更新取决于这个网页内容的变化所带来搜索质量的变化，变化越大的网页，则应该越快更新。

* 聚类抽样策略

前面两种更新策略严重依赖于网页的历史更新信息，因为这是能够进行后续计算的基础，但是现实中，为每个网页保存其历史信息，搜索系统会大量增加额外负担。聚类抽象策略认为：网页具有一些属性，根据这些属性可以预测其更新周期，具有相似属性的网页，其更新周期也是类似的，于是根据这些属性将网页归类，同一类别的网页具有相同的更新频率。

能够体现网页更新周期的属性特征划分为两大类：静态特征和动态特征。静态特征包括：页面的内容，图片数量，页面大小，链接深度，PageRank值等，动态特征则体现了静态特征随着时间的变化情况，比如图片数量的变化情况，入链出链的变化情况等，根据这两类特征，即可对网页进行聚类。

### 暗网抓取
暗网抓取为了增加网页覆盖率，暗网就是目前搜索引擎爬虫按照常规的方式很难抓取到的互联网页面。比如携程旅行网的机票数据。

### 分布式爬虫
分布式爬虫的分布式机制决定了爬虫系统的性能。

分布式爬虫主要分成三个层级：分布式数据中心，分布式抓取服务，分布式爬虫。

##### 主从式分布爬虫
有一台专门的服务器进行url分发服务，其他机器则进行实际的网页下载。url服务器维护待抓取url队列，并从中获得待抓取网页的url，分配给不同的抓取服务器

##### 对等分布式爬虫
在对等分布式爬虫系统中，服务器之间不存在分工差异，每台服务器承担相同的功能，各自负担一部分url的抓取工作。

对等式爬虫的分工可以对域名进行hash，然后分给相应的爬虫服务器，即同一个爬虫服务器负责下载相同的一个站点，也可以控制对某个网站的访问速度。由于没有url分发服务器，所以这种方法不存在系统瓶颈。

但是如果有一台机器宕机或者新加入一台机器，那么hash取膜后的值会跟着变化，这意味着几乎所有的任务都需要重新分配，无疑导致资源的浪费。为了解决这个问题，转而采用一致性哈希方法来确定服务器的任务分工。

> 一致性哈希将网站的主域名进行哈希，映射为一个范围为0-2<sup>32</sup>之间的某个数值，将哈希值范围的首位相接，即任务0和最大值重合，这样可以看作是有序的环状序列，从数值0开始，沿着环的顺时针方向，哈希值逐渐增大，知道环的结尾。而某个抓取服务器则负责这个环状序列的一个片段，即落在某个哈希取值范围内的url都由该服务器负责下载。这样即可确定每台服务器的取值范围。

![一致性哈希分布的爬虫服务器结构](../../../../img/in-post/0512/一致性哈希分布的爬虫服务器 "一致性哈希分布的爬虫服务器结构")








