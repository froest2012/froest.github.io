---
layout:     post
title:      "这就是搜索引擎第十章-网页去重"
subtitle:   "这就是搜索引擎读书笔记"
date:       2016-06-29
author:     "秀川"
header-img: "img/post-bg-re-vs-ng2.jpg"
header-mask: 0.3
catalog:    true
tags:
    - 读书笔记
    - 《这就是搜索引擎》
    - 搜索
    - 网页去重
---

# 网页去重
互联网中近似重复网页的数量占网页总数比例高达29%，索引重复的网页没有任何好处，而且会占用资源。

网页内容重复可以归纳为：

* 完全重复：两篇文档内容和格式上毫无差别
* 内容重复页面：两篇文档内容相同，但是布局不同
* 布局重复页面：两篇文档有部分重要的内容相同，并且布局格式相同
* 如果两篇文档有部分重要的内容相同，但是布局格式不同，则称为部分重复页面

如何快速准确地发现这些内容相似的网页已经成为提高搜索引擎服务质量的关键技术之一。

发现相似重复网页对于搜索引擎的好处：

1. 重复网页充数据库中去掉，节省一部分存储空间，这部分空间可以用于存放更多有效的网页内容，同时提高了搜索引擎的搜索质量和用户体验
2. 如果可以通过对以往搜集信息的分析，预先发现重复网页，在今后搜集过程中就可以避开这些网页，从而提高网页搜集速度。
3. 如果一个网页的镜像度较高，说明这个网页的内容比较受欢迎，也就预示着该网页相对重要，在搜集网页时应赋予它较高的优先级，而当搜索引擎系统在响应用户的检索请求并对输出结果排序时，应该赋予它较高的权值。
4. 如果用户点击了一个死链接，可以将用户引导到一个内容相同页面，这样可以有效的增加用户的检索体验。因而近似重复网页的及时发现有利于改善搜索引擎系统的服务质量。

重复网页检测在爬虫阶段进行的。当爬虫抓取到网页时，需要和已经建立索引内的网页进行重复判断，如果是近似重复网页，则直接将其抛弃，如果发现是全新的内容，则将其加入网页索引中。

![近似重复网页检查流程](../../../../img/in-post/0512/近似重复网页检查流程.png "近似重复网页检查流程")

## 通用去重算法框架

对于给定的文档，首先通过一定的特征抽取手段，从文档中抽取一系列能够表征文档主题内容的特征集合，保留文档重要的信息，抛弃无关紧要的信息。之所以要抛弃部分信息，主要是从计算速度角度考虑的，一般来说，抛弃的信息越多，计算速度会越快，但是如果抛弃得过多，在此步骤可能会丢失重要信息，所以不同的算法在此步骤需要作出权衡。在速度和准确性方面要通盘考虑，尽可能兼顾两者。

在将文档转换为特征集合后，很多算法可以直接进入查找相似文档的阶段，但是对于搜索引擎来说，所要处理的网页数量以亿计，算法的计算速度至关重要，为了能进一步加快计算速度，很多高校实用的算法会在特征集合的基础上，对信息进一步压缩，采用***信息指纹相关算法***，将特征集合压缩为新的数据集合，其包含的元素数量远远小于特征集合数量，有时候甚至只有唯一的一个文档指纹。在此处与在特征抽取阶段一样，有可能有信息丢失，所以也需要权衡压缩率和准确性的问题。

把文档压缩成文档指纹后，即可以通过相似性计算来判断哪些网页是近似重复网页。对于去重来说，最常用的文本相似性计算是Jaccard相似度，大部分去重算法是以此作为评估两个文档是否近似的标准。由于数量太大，计算相似性的时候，逐一比较效率太低，比较常见的策略是对文档集合进行分组，只要和分组内的网页进行一一比较即可，这样大大减少了比较次数，有效提升了系统效率。

## Shingling算法

1. 从文档中抽取能够代表文档内容的特征，形成一个特征集合
2. 根据两个文档对应特征集合的重叠程度来判断是否近似重复

用Shingling算法形成的特征集太大，效率不高

A页面的特征集合有4个元素，B页面的特征集合有5个元素，其中2个共同元素，那么这两个网页的相似性问 2/7

### 改进Shingling算法

把文档的特征集合用不同的hash函数转化成数值向量（实际中往往会使用84个不同的hash函数）。对于每一个hansh函数都会对所有的文档特征进行计算，将84个数组进一步压缩，以14个连续数值作为一块，将84个数值向量分为6块，利用6各hash函数将文档转换为6个数值向量，如果任意两个文档有两个以上的hash数值是相同的，即可认为是近似重复网页，这个技巧被称为SuperSingle。

![改进后的Singleling算法](../../../../img/in-post/0512/改进后的Singleling算法.png "改进后的Singleling算法")

1亿5千万个网页，使用改进后的Singleling算法可以在3小时内计算完毕

## I-Match算法

根据大规模语料进行统计，对语料中出现的所有单词，按照IDF值由高到低进行排序，之后去除一定比例IDF得分过高以及过低的单词，保留得分处于中间段的单词做为特征词典。

![I-Match算法流程](../../../../img/in-post/0512/I-Match算法流程.png "I-Match算法流程")

对于需要去重的网页，扫描一遍即可获得页面中出现过的所有***单词***，对这些单词，用特征词典进行过滤：保留在特征词典中出现过的单词，以此做为表达网页内容的特征，没有出现过的直接抛弃。通过这种方式，抽取出文档对应的特征，之后利用哈希函数（I-Match算法采用sha1做为哈希函数）对文档的所有特征词汇整体进行哈希计算，得到一个唯一的数值，以此哈希数值做为网页的信息指纹。

I-Match算法直接比较两个网页的信息指纹，如果相同，则被认为是近似重复网页。

I-Match算法对于文档之间单词顺序的变化并不敏感。I-Match算法把文档映射成一个单一的哈希值，以单一数值作为文档的表征，必然在计算速度上优于多值表征，因为可以避免复杂的集合运算。

### 缺点：

* I-Match对于短文本容易出现误判。
* I-Match算法不稳定

### 改进后

使用多个特征词典，从主特征词典中随机抽取很小比例的词典项，之后将其从主特征词典中抛弃，剩下的词典项构成一个辅助特征词典，如此重复若干次就可以形成若干个辅助特征词典，这些辅助特征词典和主特征词典在一起作为算法采用的多个特征词典。

## SimHash算法

SimHash算法可能是目前最优秀的去重算法之一。SimHash算法可以看做是局部敏感哈希框架的一个实现特例。

局部敏感哈希框架的特性：两个文档内容越相近，其对应的两个哈希值也越接近，所以可以将文本内容相似性转换成哈希值的相近性问题，提高计算速度，节省存储空间。

### 处理步骤

1. 文档指纹计算：将一篇文本文档转换为固定大小的二进制数值，以此作为文档的信息指纹。
2. 相似性查找：根据信息指纹来找出哪些文档是近似重复的。

### 文档指纹计算

实现步骤：

1. 从文档内容中抽取一批表征文档的特征，获得文档的特征及权值w。
2. 利用哈希函数将每个特征映射成固定长度的二进制表示，如图为长度等于6比特（实际中一般是64）的二进制向量，这样每个特征就转换为6比特二进制向量及权值。
3. 然后将权重融入到向量中，形成一个实数向量。

	二进制位中是1的地方，用w来代替，是0的地方用-w来代替，这样就讲二进制向量改为体现特征权重的实数向量
	某个特征都进行上述改写后，对所有特征的实数向量累加获得一个代表文档整体的实数向量，把对应位置的数值累加即可。
4. 再将实数向量转换为二进制向量，对应位置的实数如果是大于0的数，则用1表示，如果对应位置的实数小于等于0，则用0表示，这样就得到了文档的信息指纹，即最终的二进制数字串

![SimHash算法第一阶段流程](../../../../img/in-post/0512/SimHash算法第一阶段流程.png "SimHash算法第一阶段流程")

### 相似文档查找

对于两个文档A和B，其内容相似性可以通过比较二进制数值的差异来体现，内容越相似，则二进制数值对应位置的相同的0和者1越多，两个二进制数值不同的二进制位数称为“***海明距离***”，海明距离越大，则文档越不相似，一般对于64位二进制数来说，判断两个文档是否近似重复的标准是：海明距离是否小于3，如果两个文档的二进制数值小于等于3位不同，则判定为近似重复文档。

为了加快比较速度，SimHash算法将索引网页根据文档指纹进行分组，新网页只在部分分组内进行匹配，以减少新文档和索引网页的比较次数。

首先对64位长度的二进制数值进行分块，每16位为一块，这样每个二进制数值被划分为4块，可分别以A,B,C,D块来命名，对于海量的索引网页，依据分块进行聚类，比如对于A块来说，根据A块内16位二进制聚类，如果16位二进制都相同，则这些网页被看作是一个聚类，即一组，这样根据A块就可以将所有索引网页分成若干组。

![SimHash分块匹配](../../../../img/in-post/0512/SimHash分块匹配.png "SimHash分块匹配")

## SpotSig算法

SpotSig算法最初设计者的目的是为了解决新闻内容的近似重复判断而提出的。这个算法基于如下观察：很多新闻的主题内容大致相同，但是页面布局往往差异很大，比如很多导航链接或者广告链接及其文字。那么，新闻主题和页面布局区域在使用文字上有何种明显差异呢？一个很明显的差异是：停用词在两者中的分布是不一样的，新闻正文中停用词一般是比较均匀而频繁出现的，但是在其他区域中却很少出现停用词。SpotSig提出者觉得这个差异可以被用来识别近似重复新闻。

### 特征抽取

![特征提取实例](../../../../img/in-post/0512/特征提取实例.png "特征提取实例")

如上图所示选取单词a、an、the、is，这4个停用词做为锚点。从锚点向后顺序扫描，取固定个数的单词做为特征构成部分，比如例子中第一次出现的a，后面紧跟着单词序列“rally to kick”, 假设锚点后跟着单词个数设定为2，即取单词rally和kick做为这个锚点的特征组成部分，将停用词to过滤掉，这样a:rally:kick就组成了第一个特征。按照同样的地方可以获得文档所有的特征，组成特征集合，以此来表达这个文档。

文档特征是根据停用词来选取的，如果网页内容相同，只是布局不同，因为页面布局文字很少包含停用词，所以从页面布局内几乎不会抽出特征。也就是说，页面布局对于判断后续内容近似文档基本没有负面影响，这也是SpotSig算法一个显著的特色。

### 相似文档查找

获得文档的特征集和后，与Shingling算法类似，SpotSig采用Haccard来计算文档之间的相似性。考虑到计算量过大，SpotSig采取了两个技术手段来加速查找过程：文档分组和倒排索引。

考察Jaccard公式，可以得出结论，如果两个文档A和B，其长度相差太大，利用Jaccard公司计算出的两者的相似性一定很低，即两者不可能是近似重复文档。所以对于某个待判断文档A来说，与其相似的文档，长度一定与文档A的长度相差不远，可以根据特征集合的大小对文档集合进行分组，在分组内一一比较，以此加快查找速度。

为了更进一步加快匹配速度，SpotSig算法对于每个分组内的网页，分别建立一套倒排索引。对于每个特征，建立倒排列表，其中d代表文档编号，后面的数字代表这个特征在文档d出现的次数，即TF，同时，倒排列表项根据TF值由高到低排序。这样在计算Jacard相似度的时候，可以通过动态裁剪的方式只匹配分组的部分网页即可，不需要对分组内任意网页都计算Jaccard相似度，通过此种手段进一步加快了计算速度。

![分组k建立的倒排索引](../../../../img/in-post/0512/分组k建立的倒排索引.png "分组k建立的倒排索引")































